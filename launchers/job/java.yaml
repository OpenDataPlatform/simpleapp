apiVersion: batch/v1
kind: Job
metadata:
  name: ctemp-job-java
  namespace: spark-sapp-work
spec:
  ttlSecondsAfterFinished: 200
  backoffLimit: 0
  template:
    spec:
      serviceAccountName: spark
      restartPolicy: Never
      containers:
        - name: ctemp-job-java
          image: "ghcr.io/opendataplatform/spark-odp:3.2.1"
          imagePullPolicy: Always
          env:
            - name: SPARK_BUCKET
              value: "spark-sapp"
            - name: S3_ENDPOINT
              value: "https://n0.minio1:9000/"
            - name: S3_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  key: accessKey
                  name: s3access
            - name: S3_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  key: secretKey
                  name: s3access
            - name: NAMESPACE
              value: "spark-sapp-work"
            - name: HIVE_METASTORE_NAMESPACE
              value: spark-sapp-sys
          command:
            - "/bin/sh"
            - -c
            - |
              CONF="--conf spark.kubernetes.container.image=ghcr.io/opendataplatform/spark-odp:3.2.1"
              CONF="${CONF} --conf spark.kubernetes.container.image.pullPolicy=Always"
              CONF="${CONF} --conf spark.executor.instances=2"
              CONF="${CONF} --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark"
              CONF="${CONF} --conf spark.kubernetes.driver.limit.cores=1500m"
              CONF="${CONF} --conf spark.kubernetes.executor.limit.cores=1500m"
              CONF="${CONF} --conf spark.kubernetes.driver.request.cores=500m"
              CONF="${CONF} --conf spark.kubernetes.executor.request.cores=500m"
              CONF="${CONF} --conf spark.driver.memory=4G"
              CONF="${CONF} --conf spark.executor.memory=4G"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.connection.ssl.enabled=true"
              CONF="${CONF} --conf spark.eventLog.enabled=true"
              CONF="${CONF} --conf spark.kubernetes.file.upload.path=s3a://${SPARK_BUCKET}/shared"
              CONF="${CONF} --conf spark.kubernetes.namespace=${NAMESPACE}"
              CONF="${CONF} --conf spark.eventLog.dir=s3a://${SPARK_BUCKET}/eventlogs"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.endpoint=${S3_ENDPOINT}"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.access.key=${S3_ACCESS_KEY}"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.secret.key=${S3_SECRET_KEY}"
              CONF="${CONF} --conf spark.hive.metastore.uris=thrift://hive-metastore.${HIVE_METASTORE_NAMESPACE}.svc:9083"
              CONF="${CONF} --conf spark.sql.warehouse.dir=s3a://${SPARK_BUCKET}/warehouse"
              CONF="${CONF} --conf hive.metastore.warehouse.dir=s3a://${SPARK_BUCKET}/warehouse"
              # These are default values. Need to be redefined here as default file is overrided, by mounting /opt/spark/conf folder
              CONF="${CONF} --conf spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.committer.name=directory"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.committer.staging.tmp.path=/tmp/spark_staging"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.buffer.dir=/tmp/spark_local_buf"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.committer.staging.conflict-mode=fail"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.path.style.access=true"
              CONF="${CONF} --conf spark.hadoop.fs.s3a.fast.upload=true"
              CONF="${CONF} --conf spark.driver.host=$(hostname -I)"  # For --deploy-mode client
              JAR="https://n0.minio1:9000/spark-sapp/jars/simpleapp-0.1.0-uber.jar"
              set -x
              set -f
              /opt/spark/bin/spark-submit --master k8s://https://kubernetes.default.svc --deploy-mode client --name ctemp-job-java --class simpleapp.CreateTable $CONF  $JAR \
                --src "s3a://spark-sapp/data/city_temperature.csv"  --bucket spark-sapp --database sapp --table ctemp_job_java --datamartFolder /warehouse/sapp.db \
                --select "SELECT * FROM _src_"
