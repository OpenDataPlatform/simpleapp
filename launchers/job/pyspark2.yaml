apiVersion: batch/v1
kind: Job
metadata:
  name: ctemp-job-py2
  namespace: spark-sapp-work
spec:
  ttlSecondsAfterFinished: 200
  backoffLimit: 0
  template:
    spec:
      serviceAccountName: spark
      restartPolicy: Never
      containers:
        - name: ctemp-job-py2
          image: "ghcr.io/opendataplatform/spark-odp:3.2.1"
          imagePullPolicy: Always
          envFrom:
            - configMapRef:
                name: sapp-default
          env:
            - name: EXECUTOR_LIMIT_CORES    # Default value overriding
              value: "1800m"
            - name: S3_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  key: accessKey
                  name: s3access
            - name: S3_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  key: secretKey
                  name: s3access
            - name: DRIVER_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          command:
            - "/bin/sh"
            - -c
            - |
              PY_CODE="https://n0.minio1:9000/spark-sapp/py/create_table.py"
              . /opt/confBuilder.sh
              set -x
              set -f
              /opt/spark/bin/spark-submit --master k8s://https://kubernetes.default.svc --deploy-mode client --name ctemp-job-py2 $CONF $PY_CODE \
              --src "s3a://${SPARK_BUCKET}/data/city_temperature.csv"  --bucket ${SPARK_BUCKET} --database sapp --table ctemp_job_py2 --datamartFolder /warehouse/sapp.db \
              --select "SELECT * FROM _src_"
