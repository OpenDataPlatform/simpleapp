apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ctemp-awf-java-
  namespace: spark-sapp-work
  labels:
    workflows.argoproj.io/archive-strategy: "false"
    workflows.argoproj.io/container-runtime-executor: emissary
spec:
  entrypoint: create-table
  arguments:
    parameters:
      - name: database
        value: "sapp"
      - name: table
        value: "ctemp_awf_java"
  templates:
    - name: create-table
      inputs:
        parameters:
          - name: database
          - name: table
      serviceAccountName: spark
      script:
        image: ghcr.io/opendataplatform/spark-odp:3.2.1
        imagePullPolicy: Always
        env:
          - name: SPARK_BUCKET
            value: "spark-sapp"
          - name: S3_ENDPOINT
            value: "https://n0.minio1:9000/"
          - name: S3_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: accessKey
                name: s3access
          - name: S3_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: secretKey
                name: s3access
          - name: NAMESPACE
            value: "spark-sapp-work"
          - name: HIVE_METASTORE_NAMESPACE
            value: spark-sapp-sys
        command: [bash]
        source: |
          CONF="--conf spark.kubernetes.container.image=ghcr.io/opendataplatform/spark-odp:3.2.1"
          CONF="${CONF} --conf spark.kubernetes.container.image.pullPolicy=Always"
          CONF="${CONF} --conf spark.executor.instances=2"
          CONF="${CONF} --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark"
          CONF="${CONF} --conf spark.kubernetes.driver.limit.cores=1500m"
          CONF="${CONF} --conf spark.kubernetes.executor.limit.cores=1500m"
          CONF="${CONF} --conf spark.kubernetes.driver.request.cores=500m"
          CONF="${CONF} --conf spark.kubernetes.executor.request.cores=500m"
          CONF="${CONF} --conf spark.driver.memory=4G"
          CONF="${CONF} --conf spark.executor.memory=4G"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.connection.ssl.enabled=true"
          CONF="${CONF} --conf spark.eventLog.enabled=true"
          CONF="${CONF} --conf spark.kubernetes.file.upload.path=s3a://${SPARK_BUCKET}/shared"
          CONF="${CONF} --conf spark.kubernetes.namespace=${NAMESPACE}"
          CONF="${CONF} --conf spark.eventLog.dir=s3a://${SPARK_BUCKET}/eventlogs"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.endpoint=${S3_ENDPOINT}"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.access.key=${S3_ACCESS_KEY}"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.secret.key=${S3_SECRET_KEY}"
          CONF="${CONF} --conf spark.hive.metastore.uris=thrift://hive-metastore.${HIVE_METASTORE_NAMESPACE}.svc:9083"
          CONF="${CONF} --conf spark.sql.warehouse.dir=s3a://${SPARK_BUCKET}/warehouse"
          CONF="${CONF} --conf hive.metastore.warehouse.dir=s3a://${SPARK_BUCKET}/warehouse"
          # These are default values. Need to be redefined here as default file is overrided, by mounting /opt/spark/conf folder
          CONF="${CONF} --conf spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.committer.name=directory"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.committer.staging.tmp.path=/tmp/spark_staging"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.buffer.dir=/tmp/spark_local_buf"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.committer.staging.conflict-mode=fail"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.path.style.access=true"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.fast.upload=true"
          CONF="${CONF} --conf spark.driver.host=$(hostname -I)"  # For --deploy-mode client
          JAR="https://n0.minio1:9000/spark-sapp/jars/simpleapp-0.1.0-uber.jar"
          set -x
          set -f
          /opt/spark/bin/spark-submit --master k8s://https://kubernetes.default.svc --deploy-mode client --name ctemp-awf-java --class simpleapp.CreateTable $CONF  $JAR \
            --src "s3a://spark-sapp/data/city_temperature.csv"  --bucket spark-sapp --datamartFolder /warehouse/sapp.db \
            --database {{ inputs.parameters.database }} --table {{ inputs.parameters.table }} \
            --select "SELECT * FROM _src_"
