apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ctemp-awf-py1-
  namespace: spark-sapp-work
  labels:
    workflows.argoproj.io/archive-strategy: "false"
    workflows.argoproj.io/container-runtime-executor: emissary
spec:
  entrypoint: create-table
  arguments:
    parameters:
      - { name: src, value: "s3a://spark-sapp/data/city_temperature.csv" }
      - { name: bucket, value: "spark-sapp" }
      - { name: database, value: "sapp" }
      - { name: table, value: "ctemp_awf_py1" }
      - { name: datamartFolder, value: "/warehouse/sapp.db" }
      - { name: select, value: "SELECT * FROM _src_" }
  templates:
    - name: create-table
      inputs:
        parameters:
          - name: src
          - name: bucket
          - name: database
          - name: table
          - name: datamartFolder
          - name: select
      serviceAccountName: spark
      script:
        image: ghcr.io/opendataplatform/spark-odp:3.2.1
        imagePullPolicy: Always
        env:
          - name: SPARK_BUCKET
            value: "spark-sapp"
          - name: S3_ENDPOINT
            value: "https://n0.minio1:9000/"
          - name: S3_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: accessKey
                name: s3access
          - name: S3_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: secretKey
                name: s3access
          - name: NAMESPACE
            value: "spark-sapp-work"
          - name: HIVE_METASTORE_URI
            value: thrift://hive-metastore.spark-sapp-sys.svc:9083
          - name: DRIVER_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
        command: [bash]
        source: |
          CONF="--conf spark.kubernetes.container.image=ghcr.io/opendataplatform/spark-odp:3.2.1"
          CONF="${CONF} --conf spark.kubernetes.namespace=${NAMESPACE}"
          CONF="${CONF} --conf spark.kubernetes.container.image.pullPolicy=Always"
          CONF="${CONF} --conf spark.executor.instances=2"
          CONF="${CONF} --conf spark.kubernetes.executor.limit.cores=1500m"
          CONF="${CONF} --conf spark.kubernetes.executor.request.cores=500m"
          CONF="${CONF} --conf spark.executor.memory=4G"
          CONF="${CONF} --conf spark.kubernetes.authenticate.executor.serviceAccountName=spark"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.endpoint=${S3_ENDPOINT}"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.access.key=${S3_ACCESS_KEY}"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.secret.key=${S3_SECRET_KEY}"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.connection.ssl.enabled=true"
          CONF="${CONF} --conf spark.eventLog.enabled=true"
          CONF="${CONF} --conf spark.eventLog.dir=s3a://${SPARK_BUCKET}/eventlogs"
          CONF="${CONF} --conf spark.kubernetes.file.upload.path=s3a://${SPARK_BUCKET}/shared"
          CONF="${CONF} --conf spark.hive.metastore.uris=${HIVE_METASTORE_URI}"
          CONF="${CONF} --conf spark.sql.warehouse.dir=s3a://${SPARK_BUCKET}/warehouse"
          CONF="${CONF} --conf hive.metastore.warehouse.dir=s3a://${SPARK_BUCKET}/warehouse"
          CONF="${CONF} --conf spark.kubernetes.driver.pod.name=${DRIVER_POD_NAME}"
          # These are default values. Need to be redefined here as default file is overrided, by mounting /opt/spark/conf folder
          CONF="${CONF} --conf spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.committer.name=directory"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.committer.staging.tmp.path=/tmp/spark_staging"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.buffer.dir=/tmp/spark_local_buf"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.committer.staging.conflict-mode=fail"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.path.style.access=true"
          CONF="${CONF} --conf spark.hadoop.fs.s3a.fast.upload=true"
          CONF="${CONF} --conf spark.driver.host=$(hostname -I)"  # For --deploy-mode client
          PY_CODE="https://n0.minio1:9000/spark-sapp/py/create_table.py"
          set -x
          set -f
          /opt/spark/bin/spark-submit --master k8s://https://kubernetes.default.svc --deploy-mode client --name ctemp-awf-py1 $CONF $PY_CODE \
            --src "{{ inputs.parameters.src }}"  --bucket {{ inputs.parameters.bucket }} --datamartFolder "{{ inputs.parameters.datamartFolder }}" \
            --database {{ inputs.parameters.database }} --table {{ inputs.parameters.table }} \
            --select "{{ inputs.parameters.select }}"
